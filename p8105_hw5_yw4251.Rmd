---
title: "p8105_hw5_yw4251"
author: "You Wu"
date: 2023-11-12
output: github_document
---
# Problem 0
Load Necessary Packages.
```{r}
library(tidyverse)
library(purrr)
library(ggridges)
```

# Problem 2

Load and tidy the dataset.

```{r}
file_list = list.files(path = "data/",full.names = TRUE)
file_info_df = data.frame(file_name = file_list)

process_file = function(file_path) {
  parts = str_split(basename(file_path), "_|\\.")[[1]]
  arm = parts[1]
  subject_id = parts[2]

  data = read.csv(file_path) 

  data = mutate(data, subject_id = subject_id, arm = arm)

  return(data)
}

all_data = map_df(file_info_df$file_name, process_file)|>
  pivot_longer(
    week_1:week_8,
    names_to="week",
    values_to="outcome",
    names_prefix="week_"
  )|>
  mutate(
    week = as.numeric(week),
    arm=recode(arm, "con"="control", "exp"="experimental"),
    arm = as.factor(arm)           
  )

all_data
```

Make a spaghetti plot showing observations on each subject over time
```{r}
ggplot(all_data, aes(x = week, y = outcome, group = subject_id, color = subject_id)) +
  geom_line() +
  geom_point(size=1.5)+
  theme_minimal() +
  facet_grid(~arm)+
  labs(
    title = "Spaghetti Plot of Observations Over Time by Subjects",
    x = "Week",
    y = "Observation Outcome",
    color = "Subject ID"
  ) 
```

## _Differences between groups:_

**Trend**: The experimental group shows an increasing trend over time, suggesting a possible treatment effect, while the control group's trend is not consistently upward.

**Cohesion**: Outcomes vary significantly among subjects in both groups. The control group's outcomes are more closely bunched together, in contrast to the experimental group, where there's a broader spread of individual outcomes, suggesting a range of responses to the treatment.

**Time Change**: The distinction between the control and experimental groups grows more evident around the fourth or fifth week, suggesting this may be the period when the treatment begins to have a discernible impact.




# Problem 3

Generate 5000 datasets when `mu=0`.
```{r}
set.seed(1)
sim_mean_test <- function(mu,n=30, sigma = 5) {
  sim_data = tibble(
    x=rnorm(n=n, mean=mu, sd = sigma)
  )
  test_result = broom::tidy(t.test(sim_data$x))|>
    select(mu_hat=estimate, p.value)
  return(test_result)
}

sim_results_df_0 =   
  map(1:5000, \(i) sim_mean_test(0)) |> 
  bind_rows()

head(sim_results_df_0)

```

Repeat simulation for `mu={1,2,3,4,5,6}`.
```{r}
set.seed(1)
sim_results_df_0_6 = 
  expand_grid(
    mu = 0:6,
    iter = 1:5000
  )|> 
  mutate(
    estimate_df = map(mu, sim_mean_test)
  ) |> 
  unnest(estimate_df)

power_analysis=sim_results_df_0_6|>
  group_by(mu)|>
  summarise(
    power=mean(p.value<0.05),
    avg_mu_hat = mean(mu_hat),
    avg_mu_hat_rejected = mean(mu_hat[p.value < 0.05])
    )

```

Make plot to show the relationship between the effect size (represented by the true value of μ) and the statistical power.

```{r}
power_analysis|>
  ggplot(aes(x=mu,y=power))+
  geom_point(colour="red") +
  geom_line(colour="blue") +
  labs(title = "Statistical Power by True Mean", x = "True Mean(μ)", y = "Power")+
  theme_minimal()
```
The larger the true mean `μ`, the greater the power of the test.  Initially, the power is almost negligible, but it surges significantly as the true mean grows, eventually approaching a power value of 1. This indicates that the larger the discrepancy from the null hypothesis, the more effective the test is at identifying an actual effect, showing a positive correlation between effect size and test power.

Now, plot average estimate of u.

```{r}
ggplot(power_analysis, aes(x = mu)) +
  geom_line(aes(y = avg_mu_hat, color = "Average μ Estimate")) +
  geom_point(aes(y = avg_mu_hat, color = "Average μ Estimate") )+
  geom_line(aes(y = avg_mu_hat_rejected, color = "Average μ Estimate (Null Rejected)")) +
  geom_point(aes(y = avg_mu_hat_rejected, color = "Average μ Estimate (Null Rejected)")) +
  labs(
    title = "Average Estimates of μ", 
    x = "True Value of μ", 
    y = "Average Estimate of μ",
    color = "Estimate Type"
  ) +
  theme_minimal() +
  theme(legend.position = "bottom")+
  scale_color_manual(values = c("Average μ Estimate" = "#e7298a", "Average μ Estimate (Null Rejected)" = "#7570b3"))
```


No, the sample average of $\hat\mu$ across tests for which the null is rejected is not approximately equal to the true value of $\mu$, especially at lower true mean values. As the true mean $\mu$ increases, the average estimated $\hat\mu$ from tests where the null hypothesis is rejected approaches the actual value, due to the positive correlation between the magnitude of the effect and the power of the test.
At a true mean of 0, the average $\hat\mu$ from rejected tests is close to 0, influenced by a large number of trials and the central limit theorem. As the true mean increases, more tests reject the null hypothesis, aligning the sample mean with the true mean, especially noticeable at higher true means.

For rejected tests, the average $\hat\mu$ is typically higher than the true $\mu$, especially at smaller true mean values. This is because rejections occur when sample means significantly differ from the null hypothesis mean of zero. With larger true means, the average $\hat\mu$ aligns more closely with $\mu$.


